{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Ag-SlDfZNAtNqPD3lgW061Lqo_4q8qYx","authorship_tag":"ABX9TyNJ8saNCTH9np6stXQkmab2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8fcf57c2ddc2414c9a6c309efdf29b2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_640b1e9603754782a9dd0362a6c2579c","IPY_MODEL_81c96b37bba04c6594b40dfddee651c8","IPY_MODEL_d566f22ef6304a2fa4d31dea37aa0b2a"],"layout":"IPY_MODEL_f9486d28ab3144109d419be00731e8d3"}},"640b1e9603754782a9dd0362a6c2579c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c5d54ff810a4bbeb322e007af6f0556","placeholder":"​","style":"IPY_MODEL_07a717b5140044fa8a1bed147257e690","value":"Downloading: 100%"}},"81c96b37bba04c6594b40dfddee651c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c17d01bbfa1420084bf76ddf5293257","max":666,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2a282db1d9f4e9fb5cc966a9beb08c9","value":666}},"d566f22ef6304a2fa4d31dea37aa0b2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c785b26e8d4b2b8be93ff9ec4ad700","placeholder":"​","style":"IPY_MODEL_243d53180f6b41c89f5dfa41d5403128","value":" 666/666 [00:00&lt;00:00, 15.0kB/s]"}},"f9486d28ab3144109d419be00731e8d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c5d54ff810a4bbeb322e007af6f0556":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07a717b5140044fa8a1bed147257e690":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c17d01bbfa1420084bf76ddf5293257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2a282db1d9f4e9fb5cc966a9beb08c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9c785b26e8d4b2b8be93ff9ec4ad700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"243d53180f6b41c89f5dfa41d5403128":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd74abc3131f44b6a1f7b30340efff77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6372005659644d0fa7aac83c2a10bfb5","IPY_MODEL_9d1491663bfb4e8aa3081b0e3a7afe5f","IPY_MODEL_cae50bc406fc4e4ab5ae1eba1f4b1f8a"],"layout":"IPY_MODEL_6033dc3aee804830900d7041fab7ea29"}},"6372005659644d0fa7aac83c2a10bfb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd1ca734b0f445c95af5d864ee3b5f1","placeholder":"​","style":"IPY_MODEL_6e566974143e4c448ed55d8d3356675b","value":"Downloading: 100%"}},"9d1491663bfb4e8aa3081b0e3a7afe5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb24815fb3694d40a12a694900b02352","max":3247202234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06cbc1d972b040a18b9d6fb57ec9560f","value":3247202234}},"cae50bc406fc4e4ab5ae1eba1f4b1f8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43b1fe1773c1432cac18c01bfd48ea97","placeholder":"​","style":"IPY_MODEL_7294ecc294ab420d8ae559fe4378e2e7","value":" 3.25G/3.25G [00:55&lt;00:00, 76.2MB/s]"}},"6033dc3aee804830900d7041fab7ea29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd1ca734b0f445c95af5d864ee3b5f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e566974143e4c448ed55d8d3356675b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb24815fb3694d40a12a694900b02352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06cbc1d972b040a18b9d6fb57ec9560f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43b1fe1773c1432cac18c01bfd48ea97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7294ecc294ab420d8ae559fe4378e2e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc5619d2bee94fb4bada77c38b4431e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce67fa75e29d464ba21ff8502d694df0","IPY_MODEL_2ad2ca87bd6f45ddb6832993dc0fcfb6","IPY_MODEL_a72ee5e0fc62436591eb38398ec094e5"],"layout":"IPY_MODEL_946bafd34e2b42d0add847b26ed4f71e"}},"ce67fa75e29d464ba21ff8502d694df0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_426c084183334b47915b677b234a8558","placeholder":"​","style":"IPY_MODEL_e1560a46dc644be082a493f263275e2e","value":"Downloading: 100%"}},"2ad2ca87bd6f45ddb6832993dc0fcfb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab61d08c46a42208c98ab2a89bbbdd6","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2438cc5a0c4441d94b4338f737304ff","value":1042301}},"a72ee5e0fc62436591eb38398ec094e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a162a8ec164f75b2ccde61c2e17e1d","placeholder":"​","style":"IPY_MODEL_bbda44e0534044bf9e043d288d868322","value":" 1.04M/1.04M [00:00&lt;00:00, 3.07MB/s]"}},"946bafd34e2b42d0add847b26ed4f71e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426c084183334b47915b677b234a8558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1560a46dc644be082a493f263275e2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ab61d08c46a42208c98ab2a89bbbdd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2438cc5a0c4441d94b4338f737304ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a162a8ec164f75b2ccde61c2e17e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbda44e0534044bf9e043d288d868322":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33e25d73790e4ba583c6fd0af7f6f132":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16357a178e88492c8d1a66149c968b80","IPY_MODEL_3348329d6e29447db4130687b3a02b2d","IPY_MODEL_de460f1b67464d57bcd52b1083a643ad"],"layout":"IPY_MODEL_6ee220475b6d4e23b1e04955f3cb993f"}},"16357a178e88492c8d1a66149c968b80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffa3c6df8ea946248b1e292eb8f05af3","placeholder":"​","style":"IPY_MODEL_2bc92304ba294d9d83d83bb7f8bd77ac","value":"Downloading: 100%"}},"3348329d6e29447db4130687b3a02b2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cfbec647c53489abc9a85289b986bc3","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38b8783de106494b9e5aab5642bf8b87","value":456318}},"de460f1b67464d57bcd52b1083a643ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecefd5fd3094478aa5f74b1fa082bf97","placeholder":"​","style":"IPY_MODEL_514da9f713b74c058a79845a4f40c5fa","value":" 456k/456k [00:00&lt;00:00, 813kB/s]"}},"6ee220475b6d4e23b1e04955f3cb993f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffa3c6df8ea946248b1e292eb8f05af3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bc92304ba294d9d83d83bb7f8bd77ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cfbec647c53489abc9a85289b986bc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38b8783de106494b9e5aab5642bf8b87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecefd5fd3094478aa5f74b1fa082bf97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"514da9f713b74c058a79845a4f40c5fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98b71be335c040bf8ec753a527762f7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2280a842ba3647919bce72f13b2923b0","IPY_MODEL_c8cec4c4f36d48c48f302979422f2b3b","IPY_MODEL_5136f9585b7b4ffd8397a05518b66708"],"layout":"IPY_MODEL_5cebfe71793e456894820bb162c76723"}},"2280a842ba3647919bce72f13b2923b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5db81d63a125432f9097fc4167bd10c5","placeholder":"​","style":"IPY_MODEL_3e6b9b2475324a83ba8cfcc0d21a9bf2","value":"Downloading: 100%"}},"c8cec4c4f36d48c48f302979422f2b3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adf53678e133404792559a3bd7844ae0","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f448afeab272487883fba146253e1c9e","value":1355256}},"5136f9585b7b4ffd8397a05518b66708":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4aa638ce5c144939db9a76f27eccec7","placeholder":"​","style":"IPY_MODEL_c408a704400d4770aff8baabb8c8cc8d","value":" 1.36M/1.36M [00:00&lt;00:00, 3.09MB/s]"}},"5cebfe71793e456894820bb162c76723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5db81d63a125432f9097fc4167bd10c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e6b9b2475324a83ba8cfcc0d21a9bf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adf53678e133404792559a3bd7844ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f448afeab272487883fba146253e1c9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4aa638ce5c144939db9a76f27eccec7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c408a704400d4770aff8baabb8c8cc8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["### Enter your path for downloaded directory NL2STL\n","#home_path_nl2stl = 'YOUR-OWN-PATH/NL2STL'\n","#%cd 'YOUR-OWN-PATH/NL2STL'\n","\n","home_path_nl2stl = '/content/drive/MyDrive/Colab Notebooks/Code/NLP_robotics/LTL_dataset/github' ### Enter your path for downloaded directory NL2STL\n","%cd '/content/drive/MyDrive/Colab Notebooks/Code/NLP_robotics/LTL_dataset/github'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcXKBYa4c2Sf","executionInfo":{"status":"ok","timestamp":1670030255973,"user_tz":300,"elapsed":1592,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}},"outputId":"158cbc04-a481-40e7-e93a-50930d6b84ae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Code/NLP_robotics/LTL_dataset/github\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install SentencePiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qc4ixRSfmiqf","executionInfo":{"status":"ok","timestamp":1670005578244,"user_tz":300,"elapsed":15135,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}},"outputId":"6a9b5004-9e15-4968-f998-10501ee31f2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 46.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SentencePiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.9 MB/s \n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.1.97\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import time\n","from argparse import ArgumentParser\n","import tqdm\n","import torch\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader\n","\n","from transformers import T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n","from seq2seq import LTL2Eng\n","from data import LTLDataset, generate_vocabs, LTL2EngT5Dataset, UnlabeledLTLDataset\n","from config import Config\n","from util import collect_ltl_vocabs\n","\n","config = Config.from_json_file('eng2ltl_t5_load_data.json')\n","input_dir = os.path.join(home_path_nl2stl, 'eng2ltl_para_gen_5_11_28_word_infix')\n","\n","torch.cuda.set_device(0)\n","model_name = config.bert_model_name\n","tokenizer = T5Tokenizer.from_pretrained(model_name,cache_dir=config.bert_cache_dir)\n","\n","ltl_vocabs = collect_ltl_vocabs([home_path_nl2stl+config.train_file,home_path_nl2stl+config.dev_file,home_path_nl2stl+config.test_file])\n","model = LTL2Eng(config,ltl_vocabs,tokenizer)\n","model.load_state_dict(torch.load(input_dir+'/model_state.pt'))\n","model.cuda(device=0)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8fcf57c2ddc2414c9a6c309efdf29b2c","640b1e9603754782a9dd0362a6c2579c","81c96b37bba04c6594b40dfddee651c8","d566f22ef6304a2fa4d31dea37aa0b2a","f9486d28ab3144109d419be00731e8d3","1c5d54ff810a4bbeb322e007af6f0556","07a717b5140044fa8a1bed147257e690","0c17d01bbfa1420084bf76ddf5293257","b2a282db1d9f4e9fb5cc966a9beb08c9","f9c785b26e8d4b2b8be93ff9ec4ad700","243d53180f6b41c89f5dfa41d5403128","bd74abc3131f44b6a1f7b30340efff77","6372005659644d0fa7aac83c2a10bfb5","9d1491663bfb4e8aa3081b0e3a7afe5f","cae50bc406fc4e4ab5ae1eba1f4b1f8a","6033dc3aee804830900d7041fab7ea29","3bd1ca734b0f445c95af5d864ee3b5f1","6e566974143e4c448ed55d8d3356675b","eb24815fb3694d40a12a694900b02352","06cbc1d972b040a18b9d6fb57ec9560f","43b1fe1773c1432cac18c01bfd48ea97","7294ecc294ab420d8ae559fe4378e2e7","dc5619d2bee94fb4bada77c38b4431e2","ce67fa75e29d464ba21ff8502d694df0","2ad2ca87bd6f45ddb6832993dc0fcfb6","a72ee5e0fc62436591eb38398ec094e5","946bafd34e2b42d0add847b26ed4f71e","426c084183334b47915b677b234a8558","e1560a46dc644be082a493f263275e2e","0ab61d08c46a42208c98ab2a89bbbdd6","c2438cc5a0c4441d94b4338f737304ff","e6a162a8ec164f75b2ccde61c2e17e1d","bbda44e0534044bf9e043d288d868322","33e25d73790e4ba583c6fd0af7f6f132","16357a178e88492c8d1a66149c968b80","3348329d6e29447db4130687b3a02b2d","de460f1b67464d57bcd52b1083a643ad","6ee220475b6d4e23b1e04955f3cb993f","ffa3c6df8ea946248b1e292eb8f05af3","2bc92304ba294d9d83d83bb7f8bd77ac","4cfbec647c53489abc9a85289b986bc3","38b8783de106494b9e5aab5642bf8b87","ecefd5fd3094478aa5f74b1fa082bf97","514da9f713b74c058a79845a4f40c5fa","98b71be335c040bf8ec753a527762f7f","2280a842ba3647919bce72f13b2923b0","c8cec4c4f36d48c48f302979422f2b3b","5136f9585b7b4ffd8397a05518b66708","5cebfe71793e456894820bb162c76723","5db81d63a125432f9097fc4167bd10c5","3e6b9b2475324a83ba8cfcc0d21a9bf2","adf53678e133404792559a3bd7844ae0","f448afeab272487883fba146253e1c9e","e4aa638ce5c144939db9a76f27eccec7","c408a704400d4770aff8baabb8c8cc8d"]},"id":"2PcUg4mVzLGO","executionInfo":{"status":"ok","timestamp":1670005782406,"user_tz":300,"elapsed":204171,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}},"outputId":"a5da6d61-e5e3-4008-e719-29e8c5fd2642"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/666 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fcf57c2ddc2414c9a6c309efdf29b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd74abc3131f44b6a1f7b30340efff77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5619d2bee94fb4bada77c38b4431e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e25d73790e4ba583c6fd0af7f6f132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b71be335c040bf8ec753a527762f7f"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["LTL2Eng(\n","  (ltl2eng): T5ForConditionalGeneration(\n","    (shared): Embedding(32128, 768)\n","    (encoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 768)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","                (relative_attention_bias): Embedding(32, 12)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (2): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (3): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (4): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (5): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (6): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (7): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (8): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (9): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (10): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (11): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (decoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 768)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","                (relative_attention_bias): Embedding(32, 12)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (2): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (3): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (4): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (5): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (6): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (7): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (8): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (9): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (10): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (11): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n","  )\n","  (eng2ltl): T5ForConditionalGeneration(\n","    (shared): Embedding(32128, 768)\n","    (encoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 768)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","                (relative_attention_bias): Embedding(32, 12)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (2): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (3): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (4): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (5): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (6): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (7): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (8): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (9): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (10): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (11): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (decoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 768)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","                (relative_attention_bias): Embedding(32, 12)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (2): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (3): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (4): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (5): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (6): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (7): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (8): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (9): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (10): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (11): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerCrossAttention(\n","              (EncDecAttention): T5Attention(\n","                (q): Linear(in_features=768, out_features=768, bias=False)\n","                (k): Linear(in_features=768, out_features=768, bias=False)\n","                (v): Linear(in_features=768, out_features=768, bias=False)\n","                (o): Linear(in_features=768, out_features=768, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (2): T5LayerFF(\n","              (DenseReluDense): T5DenseGatedActDense(\n","                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): NewGELUActivation()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n","  )\n","  (gpt2): GPT2LMHeadModel(\n","    (transformer): GPT2Model(\n","      (wte): Embedding(50257, 1280)\n","      (wpe): Embedding(1024, 1280)\n","      (drop): Dropout(p=0.1, inplace=False)\n","      (h): ModuleList(\n","        (0): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (24): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (25): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (26): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (27): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (28): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (29): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (30): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (31): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (32): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (33): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (34): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (35): GPT2Block(\n","          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n","  )\n","  (ce_loss_fct): CrossEntropyLoss()\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import pandas as pd\n","# Here you can define the imperative natural sentences to express orders, like the following example\n","# Here we represent atomic propositions with ( prop_* )\n","'''\n","test_sentence = ['If ( prop_4 ) happens and implies ( prop_2 ) and this scenario continues to hold until at some point during the 243 to 582 time units ( prop_3 ) is detected , then the scenario is equivalent to ( prop_1 ) .',\n","                 'If it is not the case that ( prop_3 ) is detected for each time instant in the coming 164 to 612 time units , or else ( prop_1 ) happens , then ( prop_2 ) .',\n","                 'If at some point ( prop_3 ) and ( prop_2 ) , and is equivalent to ( prop_4 ) , and this scenario will hold until at some other point ( prop_1 ) is detected .',  \n","                 '( prop_1 ) happens implies ( prop_2 ) happens',               \n","                 \n","                 ]\n","'''\n","\n","'''\n","# Another way is to load the excel files, there are some example files in the dataset dir, also here we use one example file\n","file_name = 'example_excel_nl.xlsx'\n","df = pd.read_excel(file_name)\n","test_sentence = [df['paraphrased_logic_sentence'][i] for i in range(len(df))]\n","'''\n","\n","\n","# Another way is to load the txt file\n","my_file = open(\"example_excel_nl.txt\", \"r\")\n","# reading the file\n","data_test_sentence = my_file.read()\n","\n","# replacing end splitting the text \n","# when newline ('\\n') is seen.\n","test_sentence = data_test_sentence.split(\"\\n\")\n","my_file.close()\n"],"metadata":{"id":"Yg3VCOqNkRxv","executionInfo":{"status":"ok","timestamp":1670030543341,"user_tz":300,"elapsed":126,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","home_path_output = home_path_nl2stl + '/application_test/'\n","if not os.path.exists(home_path_output):\n","  os.mkdir(home_path_output)\n","\n","dataset_total = [];\n","with open(home_path_output+\"test1_word_infix.jsonl\", \"w\") as outfile:\n","    for i in range(len(test_sentence)):\n","      dataset_item = {};\n","      dataset_item['id'] = i\n","      dataset_item['logic_ltl'] = ''\n","      dataset_item['logic_sentence'] = test_sentence[i].split(' ')\n","      outfile.write(json.dumps(dataset_item)+'\\n')\n","outfile.close()\n","\n","test_set = LTL2EngT5Dataset(home_path_output+\"test1_word_infix.jsonl\",tokenizer,config,model.ltl2eng.config,ltl_vocabs=ltl_vocabs)\n","data_batch =  DataLoader(test_set, batch_size=1,\n","                        shuffle=False, collate_fn=test_set.collate_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImC_9EEFbS83","executionInfo":{"status":"ok","timestamp":1670005783866,"user_tz":300,"elapsed":669,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}},"outputId":"18ab8cb7-f301-4af1-a972-7f47e8bc0585"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["68it [00:00, 362.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Loaded 68 instances from /content/drive/MyDrive/Colab Notebooks/Code/NLP_robotics/LTL_dataset/github/application_test/test1_word_infix.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["pred_test = []\n","for i, batch in enumerate(data_batch):\n","  if i <len(test_sentence):\n","    outputs = model.predict_eng2ltl(batch, max_length=config.max_generate_length)\n","    pred_ltls = tokenizer.batch_decode(outputs['output_idxs'], skip_special_tokens=True)\n","    print(test_sentence[i])\n","    print(pred_ltls[0])\n","    print('\\n')\n","    pred_test.append(pred_ltls[0])\n","  else:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2DP5wIvYnVq","executionInfo":{"status":"ok","timestamp":1670008167073,"user_tz":300,"elapsed":33325,"user":{"displayName":"Yongchao Chen","userId":"06121221447580717190"}},"outputId":"e30d62e5-294c-40ce-c148-3be394e74108"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For all time instants in the future ( prop_2 ) should be detected , or ( prop_1 ) happens in one time , or else ( prop_3 ) happens in one time .\n","( ( globally ( prop_2 or prop_1 ) or prop_3 )\n","\n","\n","If ( prop_3 ) then ( prop_4 ) , then ( prop_2 ) , or else ( prop_1 ) .\n","( ( ( prop_3 imply prop_4 ) imply prop_2 ) or prop_1 )\n","\n","\n","If at some point ( prop_4 ) or ( prop_2 ) then the scenario is equivalent to ( prop_1 ) , it should be followed by ( prop_3 ) .\n","( ( ( prop_4 or prop_2 ) equal prop_1 ) imply prop_3 )\n","\n","\n","If ( prop_1 ) and ( prop_4 ) are equivalent to ( prop_3 ) , and also ( prop_2 ) .\n","( ( ( prop_1 and prop_4 ) equal prop_3 ) and prop_2 )\n","\n","\n","If ( prop_3 ) does not happen and also ( prop_1 ) happens , then ( prop_2 ) .\n","( ( negation prop_3 and prop_1 ) imply prop_2 )\n","\n","\n","If ( prop_1 ) is equivalent to ( prop_2 ) or else ( prop_3 ) and also ( prop_4 ) .\n","( ( ( prop_1 equal prop_2 ) or prop_3 ) and prop_4 )\n","\n","\n","It is required that at a certain point ( prop_2 ) and ( prop_3 ) , or else ( prop_1 ) .\n","( finally ( prop_2 and prop_3 ) or prop_1 )\n","\n","\n","If ( prop_4 ) happens and implies ( prop_2 ) and this scenario continues to hold until at some point during the 243 to 582 time units ( prop_3 ) is detected , then the scenario is equivalent to ( prop_1 ) .\n","( ( ( prop_4 imply prop_2 ) until [243,582] prop_3 ) equal prop_1 )\n","\n","\n","If ( prop_3 ) is equivalent to ( prop_2 ) then ( prop_4 ) happens and this scenario will hold until at some other point during the 115 to 541 time units ( prop_1 ) is detected .\n","( ( ( prop_3 equal prop_2 ) imply prop_4 ) until [115,541] prop_1 )\n","\n","\n","If at some point ( prop_1 ) happens and continues to happen until ( prop_2 ) is detected , and ( prop_4 ) , then ( prop_3 ) .\n","( ( ( prop_1 until prop_2 ) and prop_4 ) imply prop_3 )\n","\n","\n","If ( prop_1 ) is equivalent to ( prop_3 ) and this scenario continues until ( prop_2 ) is detected , then ( prop_4 ) .\n","( ( ( prop_1 equal prop_3 ) until prop_2 ) imply prop_4 )\n","\n","\n","For each time instant in the next 492 to 921 time units ( prop_2 ) should happen or ( prop_3 ) is detected , and also ( prop_1 ) .\n","( ( globally [492,921] prop_2 or prop_3 ) and prop_1 )\n","\n","\n","For each time instant ( prop_1 ) is equivalent to ( prop_2 ) , or else ( prop_3 ) .\n","( globally ( prop_1 equal prop_2 ) or prop_3 )\n","\n","\n","If it is not the case that ( prop_3 ) is detected for each time instant in the coming 164 to 612 time units , or else ( prop_1 ) happens , then ( prop_2 ) .\n","( ( negation prop_3 imply prop_1 ) or prop_2 )\n","\n","\n","It is not the case that ( prop_2 ) and ( prop_1 ) is detected and this scenario will hold until ( prop_3 ) .\n","( negation ( prop_2 and prop_1 ) until prop_3 )\n","\n","\n","For each time instant in the future it should be the case that if ( prop_3 ) then ( prop_2 ) happens , and also ( prop_1 ) .\n","( globally ( prop_3 imply prop_2 ) and prop_1 )\n","\n","\n","It is required that at a certain point within the next 59 to 235 time units either ( prop_1 ) or ( prop_2 ) should be detected , or else ( prop_3 ) .\n","( finally [59,235] ( prop_1 or prop_2 ) or prop_3 )\n","\n","\n","For each time instant in the next 450 to 560 time units ( prop_3 ) is equivalent to ( prop_1 ) , and then ( prop_2 ) .\n","( globally [450,560] ( prop_3 equal prop_1 ) and prop_2 )\n","\n","\n","For each time instant in the next 496 to 504 time units either ( prop_3 ) or ( prop_2 ) should be detected , and also ( prop_1 ) .\n","( globally [496,504] ( prop_3 or prop_2 ) and prop_1 )\n","\n","\n","If ( prop_2 ) happens and continues to happen until at some point during the 176 to 415 time units that ( prop_1 ) , and also if ( prop_3 ) , then the scenario is equivalent to ( prop_4 ) .\n","( ( ( prop_2 until [176,415] prop_1 ) and prop_3 ) equal prop_4 )\n","\n","\n","If ( prop_1 ) happens and continues to happen until at some point during the 111 to 293 time units ( prop_3 ) is detected , then imply ( prop_2 ) , the above scenario is equivalent to ( prop_4 ) .\n","( ( ( prop_1 until [111,293] prop_3 ) imply prop_2 ) equal prop_4 )\n","\n","\n","If ( prop_4 ) is equivalent to ( prop_1 ) or else ( prop_2 ) or else ( prop_3 ) .\n","( ( ( prop_4 equal prop_1 ) or prop_2 ) or prop_3 )\n","\n","\n","If at some point ( prop_4 ) happens and continues to happen until ( prop_3 ) is detected , or else ( prop_2 ) , then ( prop_1 ) .\n","( ( ( prop_4 until prop_3 ) or prop_2 ) imply prop_1 )\n","\n","\n","If ( prop_3 ) does not happen is equivalent to ( prop_1 ) , then ( prop_2 ) .\n","( ( negation prop_3 equal prop_1 ) imply prop_2 )\n","\n","\n","If finally that ( prop_2 ) is detected then ( prop_1 ) , then ( prop_3 ) .\n","( ( finally ( prop_2 imply prop_1 ) imply prop_3 )\n","\n","\n","If it is not the case that ( prop_1 ) then ( prop_3 ) , then ( prop_2 ) .\n","( negation ( prop_1 imply prop_3 ) imply prop_2 )\n","\n","\n","For each time instant in the next 348 to 409 time units ( prop_2 ) continues to happen , and then ( prop_3 ) , or else ( prop_1 ) .\n","( ( globally [348,409] prop_2 imply prop_3 ) or prop_1 )\n","\n","\n","If at some point ( prop_1 ) or ( prop_4 ) is equivalent to ( prop_2 ) then this scenario will hold until at some other point during the 270 to 503 time units ( prop_3 ) is detected .\n","( ( ( prop_1 or prop_4 ) equal prop_2 ) until [270,503] prop_3 )\n","\n","\n","( prop_2 ) should not happen until at some point during the 68 to 375 time units ( prop_1 ) is detected , and the above condition is equivalent to ( prop_3 ) .\n","( ( negation prop_2 until [68,375] prop_1 ) equal prop_3 )\n","\n","\n","If ( prop_2 ) and ( prop_4 ) is equivalent to ( prop_3 ) and also ( prop_1 ) .\n","( ( ( prop_2 and prop_4 ) equal prop_3 ) and prop_1 )\n","\n","\n","If it is not the case that at each time instant in the next 253 to 441 time units ( prop_3 ) , then ( prop_2 ) , the above scenario is equivalent to ( prop_1 ) .\n","( ( negation ( globally [253,441] prop_3 ) imply prop_2 ) equal prop_1 )\n","\n","\n","In case that ( prop_4 ) happens and continues to happen until at some point during the 325 to 722 time units ( prop_3 ) is detected , this is equivalent to the scenario in which ( prop_2 ) happens , or else ( prop_1 ) .\n","( ( ( prop_4 until [325,722] prop_3 ) equal prop_2 ) or prop_1 )\n","\n","\n","It is not the case that ( prop_1 ) happens and continues to happen until at some point during the 278 to 585 time units ( prop_3 ) is detected , or else ( prop_2 ) .\n","( negation ( prop_1 until [278,585] prop_3 ) or prop_2 )\n","\n","\n","( prop_3 ) does not happen is equivalent to ( prop_1 ) happens , or else ( prop_2 ) .\n","( ( negation prop_3 equal prop_1 ) or prop_2 )\n","\n","\n","For each time instant in the next 86 to 251 time units either ( prop_2 ) or ( prop_1 ) should be detected , and also ( prop_3 ) .\n","( globally [86,251] ( prop_2 or prop_1 ) and prop_3 )\n","\n","\n","Either it is not the case that ( prop_2 ) or else ( prop_1 ) , or else ( prop_3 ) .\n","( negation ( prop_2 or prop_1 ) or prop_3 )\n","\n","\n","The scenario in which ( prop_1 ) and ( prop_4 ) and ( prop_3 ) is equivalent to the scenario in which ( prop_2 ) .\n","( ( ( prop_1 and prop_4 ) and prop_3 ) equal prop_2 )\n","\n","\n","If ( prop_1 ) is equivalent to ( prop_3 ) and this scenario will hold until at some other point during the 431 to 557 time units that ( prop_2 ) is detected , and ( prop_4 ) .\n","( ( ( prop_1 equal prop_3 ) until [431,557] prop_2 ) and prop_4 )\n","\n","\n","( prop_3 ) does not happen and ( prop_1 ) happens , and this scenario will hold until at some other point during the 394 to 612 time units ( prop_2 ) is detected .\n","( ( negation prop_3 and prop_1 ) until [394,612] prop_2 )\n","\n","\n","For each time instant in the future if ( prop_1 ) and ( prop_3 ) , then ( prop_2 ) .\n","( globally ( prop_1 and prop_3 ) imply prop_2 )\n","\n","\n","At some point during the 355 to 471 time units it should be the case that ( prop_1 ) or ( prop_2 ) is detected if and only if ( prop_3 ) .\n","( finally [355,471] ( prop_1 or prop_2 ) equal prop_3 )\n","\n","\n","If ( prop_4 ) then ( prop_1 ) happens and this scenario implies ( prop_2 ) , the above scenario will hold until at some other point during the 345 to 541 time units ( prop_3 ) is detected .\n","( ( ( prop_4 imply prop_1 ) imply prop_2 ) until [345,541] prop_3 )\n","\n","\n","If ( prop_1 ) is equivalent to ( prop_3 ) and this scenario continues to hold until at some point during the 85 to 154 time units ( prop_4 ) is detected , or else ( prop_2 ) .\n","( ( ( prop_1 equal prop_3 ) until [85,154] prop_4 ) or prop_2 )\n","\n","\n","If ( prop_3 ) is equivalent to ( prop_2 ) then ( prop_4 ) happens and this scenario will hold until ( prop_1 ) is detected .\n","( ( ( prop_3 equal prop_2 ) imply prop_4 ) until prop_1 )\n","\n","\n","If at some point ( prop_3 ) is equivalent to ( prop_4 ) and this scenario continues until at some other point during the 333 to 608 time units that ( prop_1 ) is detected , then ( prop_2 ) .\n","( ( ( prop_3 equal prop_4 ) until [333,608] prop_1 ) imply prop_2 )\n","\n","\n","For each time instant in the next 68 to 386 time units ( prop_3 ) is equivalent to ( prop_2 ) , or else ( prop_1 ) .\n","( globally [68,386] ( prop_3 equal prop_2 ) or prop_1 )\n","\n","\n","It is not the case that ( prop_3 ) and ( prop_1 ) is detected and this scenario will hold until ( prop_2 ) .\n","( negation ( prop_3 and prop_1 ) until prop_2 )\n","\n","\n","It is required that at a certain point within the next 307 to 444 time units , the scenario in which ( prop_3 ) is not detected and also ( prop_2 ) is detected , and also ( prop_1 ) .\n","( finally [307,444] ( negation prop_3 and prop_2 ) and prop_1 )\n","\n","\n","If ( prop_2 ) happens and continues to happen until at some point during the 226 to 711 time units ( prop_4 ) is detected , then ( prop_1 ) , and ( prop_3 ) .\n","( ( ( prop_2 until [226,711] prop_4 ) imply prop_1 ) and prop_3 )\n","\n","\n","It is required that at some point during the next 494 to 860 time units the scenario in which ( prop_2 ) is equivalent to the scenario in which ( prop_1 ) happens , and only then ( prop_3 ) .\n","( finally [494,860] ( prop_2 equal prop_1 ) imply prop_3 )\n","\n","\n","If at some point ( prop_3 ) then ( prop_2 ) happens and this scenario will hold until ( prop_1 ) , or else ( prop_4 ) .\n","( ( ( prop_3 imply prop_2 ) until prop_1 ) or prop_4 )\n","\n","\n","If it is not the case that ( prop_3 ) or ( prop_2 ) , then ( prop_1 ) .\n","( negation ( prop_3 or prop_2 ) imply prop_1 )\n","\n","\n","It is the case that ( prop_2 ) then ( prop_1 ) , and the above scenario is equivalent to ( prop_4 ) , or else ( prop_3 ) .\n","( ( ( prop_2 imply prop_1 ) equal prop_4 ) or prop_3 )\n","\n","\n","If it is not the case that ( prop_1 ) and ( prop_2 ) , then ( prop_3 ) .\n","( negation ( prop_1 and prop_2 ) imply prop_3 )\n","\n","\n","If at some point ( prop_4 ) happens and continues to happen until ( prop_1 ) is detected , then the scenario is equivalent to ( prop_2 ) , and ( prop_3 ) .\n","( ( ( prop_4 until prop_1 ) equal prop_2 ) and prop_3 )\n","\n","\n","For each time instant in the next 223 to 522 time units ( prop_3 ) , and ( prop_2 ) , and ( prop_1 ) .\n","( ( globally [223,522] prop_3 and prop_2 ) and prop_1 )\n","\n","\n","It is not the case that ( prop_3 ) or ( prop_1 ) happens and this scenario will hold until at some point during the 434 to 782 time units ( prop_2 ) is detected .\n","( negation ( prop_3 or prop_1 ) until [434,782] prop_2 )\n","\n","\n","In case that ( prop_2 ) and ( prop_1 ) and ( prop_3 ) happens and continues to happen until at some other point ( prop_4 ) .\n","( ( ( prop_2 and prop_1 ) and prop_3 ) until prop_4 )\n","\n","\n","If ( prop_2 ) then ( prop_4 ) happens at some point , or else ( prop_3 ) , and also ( prop_1 ) .\n","( ( ( prop_2 imply prop_4 ) or prop_3 ) and prop_1 )\n","\n","\n","If ( prop_1 ) or ( prop_2 ) then ( prop_4 ) , the above condition is equivalent to ( prop_3 ) .\n","( ( ( prop_1 or prop_2 ) imply prop_4 ) equal prop_3 )\n","\n","\n","Either ( prop_3 ) or ( prop_2 ) should not happen , and ( prop_1 ) .\n","( negation ( prop_3 or prop_2 ) and prop_1 )\n","\n","\n","If ( prop_2 ) or ( prop_1 ) happens and continues to happen until ( prop_4 ) is detected , then the scenario is equivalent to ( prop_3 ) .\n","( ( ( prop_2 or prop_1 ) until prop_4 ) equal prop_3 )\n","\n","\n","If at some point ( prop_1 ) then ( prop_4 ) and this scenario will hold until at some other point during the 102 to 542 time units ( prop_3 ) is detected , or else ( prop_2 ) .\n","( ( ( prop_1 imply prop_4 ) until [102,542] prop_3 ) or prop_2 )\n","\n","\n","( prop_3 ) is not equivalent to ( prop_1 ) , and ( prop_2 ) .\n","( ( negation prop_3 equal prop_1 ) and prop_2 )\n","\n","\n","For each time instant ( prop_3 ) and ( prop_1 ) should be detected , or else ( prop_2 ) .\n","( globally ( prop_3 and prop_1 ) or prop_2 )\n","\n","\n","If at some point ( prop_3 ) and ( prop_2 ) , and is equivalent to ( prop_4 ) , and this scenario will hold until at some other point ( prop_1 ) is detected .\n","( ( ( prop_3 and prop_2 ) equal prop_4 ) until prop_1 )\n","\n","\n","If ( prop_2 ) is equivalent to ( prop_4 ) then ( prop_1 ) happens and this scenario will hold until ( prop_3 ) .\n","( ( ( prop_2 equal prop_4 ) imply prop_1 ) until prop_3 )\n","\n","\n","If at some point ( prop_3 ) and ( prop_1 ) , and this scenario continues until at some other point during the 352 to 559 time units that ( prop_2 ) is detected , then ( prop_4 ) .\n","( ( ( prop_3 and prop_1 ) until [352,559] prop_2 ) imply prop_4 )\n","\n","\n"]}]}]}